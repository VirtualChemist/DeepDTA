{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, re, math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from matplotlib.pyplot import cm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from run_experiments import *\n",
    "from datahelper import *\n",
    "import keras.metrics\n",
    "keras.metrics.cindex_score = cindex_score\n",
    "from keras.models import load_model \n",
    "fpath = '../data/davis/'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligands = json.load(open(fpath+\"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
    "proteins = json.load(open(fpath+\"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
    "\n",
    "Y = pickle.load(open(fpath + \"Y\",\"rb\"), encoding='latin1') ### TODO: read from raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfmeasure = get_cindex\n",
    "runmethod = build_combined_categorical\n",
    "FLAGS = argparser()\n",
    "FLAGS.log_dir = FLAGS.log_dir + str(time.time()) + \"/\"\n",
    "FLAGS.num_windows = 32\n",
    "FLAGS.seq_window_lengths = [4]\n",
    "FLAGS.smi_window_lengths = [4]\n",
    "FLAGS.batch_size = 512\n",
    "FLAGS.num_epoch = 1000\n",
    "FLAGS.max_seq_len = 1000\n",
    "FLAGS.max_smi_len = 100\n",
    "FLAGS.dataset_path= '../data/davis/'\n",
    "FLAGS.problem_type = 1\n",
    "FLAGS.log_dir = '../logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../data/davis/ start\n",
      "Read ../data/davis/ start\n",
      "68\n",
      "442\n",
      "Reading ../data/davis/ start\n"
     ]
    }
   ],
   "source": [
    "dataset = DataSet( fpath = FLAGS.dataset_path, ### BUNU ARGS DA GUNCELLE\n",
    "                      setting_no = FLAGS.problem_type, ##BUNU ARGS A EKLE\n",
    "                      seqlen = FLAGS.max_seq_len,\n",
    "                      smilen = FLAGS.max_smi_len,\n",
    "                      need_shuffle = False )\n",
    "# set character set size\n",
    "FLAGS.charseqset_size = dataset.charseqset_size \n",
    "FLAGS.charsmiset_size = dataset.charsmiset_size \n",
    "\n",
    "XD, XT, Y = dataset.parse_data(fpath = FLAGS.dataset_path)\n",
    "\n",
    "XD = np.asarray(XD)\n",
    "XT = np.asarray(XT)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "drugcount = XD.shape[0]\n",
    "print(drugcount)\n",
    "targetcount = XT.shape[0]\n",
    "print(targetcount)\n",
    "\n",
    "FLAGS.drug_count = drugcount\n",
    "FLAGS.target_count = targetcount\n",
    "\n",
    "label_row_inds, label_col_inds = np.where(np.isnan(Y)==False)  #basically finds the point address of affinity [x,y]\n",
    "\n",
    "if not os.path.exists(figdir):\n",
    "    os.makedirs(figdir)\n",
    "test_set, outer_train_sets = dataset.read_sets(FLAGS.dataset_path, FLAGS.problem_type) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging(\"---Parameter Search-----\", FLAGS)\n",
    "\n",
    "Y = np.mat(np.copy(Y))\n",
    "\n",
    "params = {}\n",
    "\n",
    "XD = XD[label_row_inds]\n",
    "XT = XT[label_col_inds]\n",
    "\n",
    "train_drugs, train_prots,  train_Y = prepare_interaction_pairs(XD, XT, Y, label_row_inds, label_col_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='../../data/'\n",
    "XD_dtc, XT_dtc, Y_dtc = get_DTC_train(data_path+'dtc_for_deepDTA.csv', FLAGS.max_smi_len, FLAGS.max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "all_train_drugs = np.concatenate((np.asarray(train_drugs), np.asarray(XD_dtc)), axis=0)\n",
    "all_train_prots = np.concatenate((np.asarray(train_prots), np.asarray(XT_dtc)), axis=0)\n",
    "all_train_Y = np.concatenate((np.asarray(train_Y), np.asarray(Y_dtc)), axis=0)\n",
    "all_train_Y = -np.log10(1e-8+all_train_Y/1e9)\n",
    "\n",
    "unique_drugs = np.unique(all_train_drugs, axis=0)\n",
    "np.random.seed(15)\n",
    "choose_70_drugs = np.random.choice(np.arange(unique_drugs.shape[0]), 70, replace=False)\n",
    "val_inds = []\n",
    "for drug_ind in choose_70_drugs:\n",
    "    val_inds += list(np.where((~(all_train_drugs==unique_drugs[drug_ind, :])).sum(axis=1) == 0)[0])\n",
    "\n",
    "train_inds = np.delete(np.arange(all_train_drugs.shape[0]), val_inds)\n",
    "\n",
    "    \n",
    "XD_train, XD_val = all_train_drugs[train_inds, :], all_train_drugs[val_inds, :]\n",
    "XT_train, XT_val = all_train_prots[train_inds, :], all_train_prots[val_inds, :]\n",
    "Y_train, Y_val = all_train_Y[train_inds], all_train_Y[val_inds]\n",
    "\n",
    "#XD_train, XD_val, XT_train, XT_val, Y_train, Y_val = train_test_split(all_train_drugs, all_train_prots, all_train_Y, \n",
    "#                                                                      test_size=0.1, \n",
    "#                                                                     random_state=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ibrahim/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ibrahim/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 53955 samples, validate on 3151 samples\n",
      "Epoch 1/1000\n",
      "Epoch 00001: val_loss improved from inf to 4.17933, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 45s - loss: 1.3506 - cindex_score: 0.6414 - val_loss: 4.1793 - val_cindex_score: 0.5322\n",
      "Epoch 2/1000\n",
      "Epoch 00002: val_loss improved from 4.17933 to 1.45897, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.6143 - cindex_score: 0.6864 - val_loss: 1.4590 - val_cindex_score: 0.4538\n",
      "Epoch 3/1000\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 38s - loss: 0.5202 - cindex_score: 0.7206 - val_loss: 3.0929 - val_cindex_score: 0.5599\n",
      "Epoch 4/1000\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 38s - loss: 0.5074 - cindex_score: 0.7260 - val_loss: 1.6836 - val_cindex_score: 0.5947\n",
      "Epoch 5/1000\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 38s - loss: 0.4911 - cindex_score: 0.7361 - val_loss: 2.2725 - val_cindex_score: 0.4757\n",
      "Epoch 6/1000\n",
      "Epoch 00006: val_loss improved from 1.45897 to 0.90968, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.4913 - cindex_score: 0.7381 - val_loss: 0.9097 - val_cindex_score: 0.5253\n",
      "Epoch 7/1000\n",
      "Epoch 00007: val_loss improved from 0.90968 to 0.89244, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.4597 - cindex_score: 0.7547 - val_loss: 0.8924 - val_cindex_score: 0.6610\n",
      "Epoch 8/1000\n",
      "Epoch 00008: val_loss improved from 0.89244 to 0.70420, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.4352 - cindex_score: 0.7618 - val_loss: 0.7042 - val_cindex_score: 0.7304\n",
      "Epoch 9/1000\n",
      "Epoch 00009: val_loss improved from 0.70420 to 0.67765, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.4383 - cindex_score: 0.7611 - val_loss: 0.6777 - val_cindex_score: 0.7567\n",
      "Epoch 10/1000\n",
      "Epoch 00010: val_loss improved from 0.67765 to 0.67476, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.4121 - cindex_score: 0.7710 - val_loss: 0.6748 - val_cindex_score: 0.7460\n",
      "Epoch 11/1000\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 38s - loss: 0.4214 - cindex_score: 0.7699 - val_loss: 0.6849 - val_cindex_score: 0.7579\n",
      "Epoch 12/1000\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 38s - loss: 0.4113 - cindex_score: 0.7759 - val_loss: 0.6947 - val_cindex_score: 0.7561\n",
      "Epoch 13/1000\n",
      "Epoch 00013: val_loss improved from 0.67476 to 0.64903, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.4053 - cindex_score: 0.7789 - val_loss: 0.6490 - val_cindex_score: 0.7566\n",
      "Epoch 14/1000\n",
      "Epoch 00014: val_loss improved from 0.64903 to 0.60115, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 0.4034 - cindex_score: 0.7793 - val_loss: 0.6011 - val_cindex_score: 0.7537\n",
      "Epoch 15/1000\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 38s - loss: 0.4071 - cindex_score: 0.7813 - val_loss: 0.7264 - val_cindex_score: 0.7193\n",
      "Epoch 16/1000\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 38s - loss: 0.3889 - cindex_score: 0.7862 - val_loss: 0.6677 - val_cindex_score: 0.7515\n",
      "Epoch 17/1000\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 38s - loss: 0.3930 - cindex_score: 0.7845 - val_loss: 0.6780 - val_cindex_score: 0.7327\n",
      "Epoch 18/1000\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 38s - loss: 0.3948 - cindex_score: 0.7842 - val_loss: 0.6425 - val_cindex_score: 0.7664\n",
      "Epoch 19/1000\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 37s - loss: 0.3943 - cindex_score: 0.7837 - val_loss: 0.6956 - val_cindex_score: 0.7278\n",
      "Epoch 20/1000\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 38s - loss: 0.3893 - cindex_score: 0.7885 - val_loss: 0.6732 - val_cindex_score: 0.7162\n",
      "Epoch 21/1000\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 37s - loss: 0.3841 - cindex_score: 0.7889 - val_loss: 0.6593 - val_cindex_score: 0.7322\n",
      "Epoch 22/1000\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 38s - loss: 0.3759 - cindex_score: 0.7931 - val_loss: 0.6721 - val_cindex_score: 0.7195\n",
      "Epoch 23/1000\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 38s - loss: 0.3872 - cindex_score: 0.7896 - val_loss: 0.6172 - val_cindex_score: 0.7445\n",
      "Epoch 24/1000\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 37s - loss: 0.3721 - cindex_score: 0.7969 - val_loss: 0.6401 - val_cindex_score: 0.7492\n",
      "Epoch 25/1000\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 38s - loss: 0.3721 - cindex_score: 0.7985 - val_loss: 0.6686 - val_cindex_score: 0.7487\n",
      "Epoch 26/1000\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 37s - loss: 0.3699 - cindex_score: 0.7979 - val_loss: 0.6571 - val_cindex_score: 0.7466\n",
      "Epoch 27/1000\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 38s - loss: 0.3681 - cindex_score: 0.7985 - val_loss: 0.6496 - val_cindex_score: 0.7266\n",
      "Epoch 28/1000\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 38s - loss: 0.3749 - cindex_score: 0.7966 - val_loss: 0.6920 - val_cindex_score: 0.7192\n",
      "Epoch 29/1000\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 37s - loss: 0.3737 - cindex_score: 0.7972 - val_loss: 0.9303 - val_cindex_score: 0.7079\n",
      "Epoch 30/1000\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 38s - loss: 0.3714 - cindex_score: 0.7986 - val_loss: 0.8380 - val_cindex_score: 0.7057\n",
      "Epoch 31/1000\n",
      "Epoch 00031: val_loss did not improve\n",
      " - 37s - loss: 0.3792 - cindex_score: 0.7947 - val_loss: 0.7948 - val_cindex_score: 0.7413\n",
      "Epoch 32/1000\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 38s - loss: 0.3720 - cindex_score: 0.7994 - val_loss: 0.7533 - val_cindex_score: 0.7190\n",
      "Epoch 33/1000\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 38s - loss: 0.3639 - cindex_score: 0.8037 - val_loss: 0.8130 - val_cindex_score: 0.7278\n",
      "Epoch 34/1000\n",
      "Epoch 00034: val_loss did not improve\n",
      " - 37s - loss: 0.3631 - cindex_score: 0.8032 - val_loss: 0.7826 - val_cindex_score: 0.7261\n",
      "Epoch 35/1000\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 38s - loss: 0.3682 - cindex_score: 0.8033 - val_loss: 0.8935 - val_cindex_score: 0.7010\n",
      "Epoch 36/1000\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 37s - loss: 0.3672 - cindex_score: 0.8045 - val_loss: 0.7112 - val_cindex_score: 0.7352\n",
      "Epoch 37/1000\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 38s - loss: 0.3639 - cindex_score: 0.8035 - val_loss: 0.8033 - val_cindex_score: 0.7089\n",
      "Epoch 38/1000\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 38s - loss: 0.3619 - cindex_score: 0.8040 - val_loss: 0.7089 - val_cindex_score: 0.7341\n",
      "Epoch 39/1000\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 37s - loss: 0.3592 - cindex_score: 0.8063 - val_loss: 0.7540 - val_cindex_score: 0.7475\n"
     ]
    }
   ],
   "source": [
    "model_name='checkpoints/davis_dtc_dta'\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=25)\n",
    "checkpoint_callback = ModelCheckpoint(model_name+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "gridmodel = build_combined_categorical(FLAGS, FLAGS.num_windows, FLAGS.smi_window_lengths[0], FLAGS.seq_window_lengths[0])\n",
    "gridres = gridmodel.fit(([XD_train, XT_train ]), Y_train, batch_size=FLAGS.batch_size, epochs=FLAGS.num_epoch, \n",
    "        validation_data=( ([np.array(XD_val), np.array(XT_val) ]), np.array(Y_val))\n",
    "           , callbacks=[early_stopping_callback, checkpoint_callback], verbose=2)\n",
    "\n",
    "gridmodel.save('davis_dtc_dta.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, auc, f1_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "gridmodel = load_model('checkpoints/davis_dtc_dta.h5')\n",
    "predicted_labels = gridmodel.predict([np.array(XD_val), np.array(XT_val) ])\n",
    "loss, rperf2 = gridmodel.evaluate(([np.array(XD_val),np.array(XT_val) ]), np.array(Y_val), verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cindex: [0.79595819]\n",
      "rmse: 0.7753355061972348\n",
      "f1: 0.6260990336999411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "predicted_labels = gridmodel.predict([np.array(XD_val), np.array(XT_val) ])\n",
    "print('cindex:', get_cindex(Y_val, predicted_labels))\n",
    "print('rmse:', np.sqrt(mean_squared_error(Y_val, predicted_labels)))\n",
    "#print('pearsonr:', pearsonr(Y_val, predicted_labels[:, 0]))\n",
    "#print('spearmanr:', np.sqrt(spearmanr(val_Y, predicted_labels[:, 0])))\n",
    "print('f1:', np.sqrt(f1_score(Y_val>7, predicted_labels>7)))\n",
    "#cindex: [0.81675458]\n",
    "#rmse: 0.8683109279804\n",
    "#f1: 0.6599120175960898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XD_test, XT_test = get_DTC_train(data_path+'dtc_test_for_deepDTA.csv', FLAGS.max_smi_len, FLAGS.max_seq_len, with_label=False)\n",
    "XD_test, XT_test = np.asarray(XD_test), np.asarray(XT_test)\n",
    "predicted_labels = gridmodel.predict([np.array(XD_test), np.array(XT_test) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.read_csv(data_path+'round_1_template.csv')\n",
    "submission_data.loc[:, 'pKd_[M]_pred'] = predicted_labels\n",
    "submission_data.to_csv(data_path+'submission_file.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
