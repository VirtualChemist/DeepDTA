{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, re, math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from matplotlib.pyplot import cm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from run_experiments import *\n",
    "from datahelper import *\n",
    "import keras.metrics\n",
    "keras.metrics.cindex_score = cindex_score\n",
    "from keras.models import load_model \n",
    "fpath = '../data/davis/'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligands = json.load(open(fpath+\"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
    "proteins = json.load(open(fpath+\"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
    "\n",
    "Y = pickle.load(open(fpath + \"Y\",\"rb\"), encoding='latin1') ### TODO: read from raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfmeasure = get_cindex\n",
    "runmethod = build_combined_categorical\n",
    "FLAGS = argparser()\n",
    "FLAGS.log_dir = FLAGS.log_dir + str(time.time()) + \"/\"\n",
    "FLAGS.num_windows = 32\n",
    "FLAGS.seq_window_lengths = [4]\n",
    "FLAGS.smi_window_lengths = [4]\n",
    "FLAGS.batch_size = 512\n",
    "FLAGS.num_epoch = 1000\n",
    "FLAGS.max_seq_len = 1000\n",
    "FLAGS.max_smi_len = 100\n",
    "FLAGS.dataset_path= '../data/davis/'\n",
    "FLAGS.problem_type = 1\n",
    "FLAGS.log_dir = '../logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../data/davis/ start\n",
      "Read ../data/davis/ start\n",
      "68\n",
      "442\n",
      "Reading ../data/davis/ start\n"
     ]
    }
   ],
   "source": [
    "dataset = DataSet( fpath = FLAGS.dataset_path, ### BUNU ARGS DA GUNCELLE\n",
    "                      setting_no = FLAGS.problem_type, ##BUNU ARGS A EKLE\n",
    "                      seqlen = FLAGS.max_seq_len,\n",
    "                      smilen = FLAGS.max_smi_len,\n",
    "                      need_shuffle = False )\n",
    "# set character set size\n",
    "FLAGS.charseqset_size = dataset.charseqset_size \n",
    "FLAGS.charsmiset_size = dataset.charsmiset_size \n",
    "\n",
    "XD, XT, Y = dataset.parse_data(fpath = FLAGS.dataset_path)\n",
    "\n",
    "XD = np.asarray(XD)\n",
    "XT = np.asarray(XT)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "drugcount = XD.shape[0]\n",
    "print(drugcount)\n",
    "targetcount = XT.shape[0]\n",
    "print(targetcount)\n",
    "\n",
    "FLAGS.drug_count = drugcount\n",
    "FLAGS.target_count = targetcount\n",
    "\n",
    "label_row_inds, label_col_inds = np.where(np.isnan(Y)==False)  #basically finds the point address of affinity [x,y]\n",
    "\n",
    "if not os.path.exists(figdir):\n",
    "    os.makedirs(figdir)\n",
    "test_set, outer_train_sets = dataset.read_sets(FLAGS.dataset_path, FLAGS.problem_type) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging(\"---Parameter Search-----\", FLAGS)\n",
    "\n",
    "Y = np.mat(np.copy(Y))\n",
    "\n",
    "params = {}\n",
    "\n",
    "XD = XD[label_row_inds]\n",
    "XT = XT[label_col_inds]\n",
    "\n",
    "train_drugs, train_prots,  train_Y = prepare_interaction_pairs(XD, XT, Y, label_row_inds, label_col_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='../../data/'\n",
    "XD_dtc, XT_dtc, Y_dtc = get_DTC_train(data_path+'dtc_for_deepDTA.csv', FLAGS.max_smi_len, FLAGS.max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "all_train_drugs = np.concatenate((np.asarray(train_drugs), np.asarray(XD_dtc)), axis=0)\n",
    "all_train_prots = np.concatenate((np.asarray(train_prots), np.asarray(XT_dtc)), axis=0)\n",
    "all_train_Y = np.concatenate((np.asarray(train_Y), np.asarray(Y_dtc)), axis=0)\n",
    "all_train_Y = -np.log10(1e-8+all_train_Y/1e9)\n",
    "\n",
    "unique_drugs = np.unique(all_train_drugs, axis=0)\n",
    "np.random.seed(15)\n",
    "choose_70_drugs = np.random.choice(np.arange(unique_drugs.shape[0]), 70, replace=False)\n",
    "val_inds = []\n",
    "for drug_ind in choose_70_drugs:\n",
    "    val_inds += list(np.where((~(all_train_drugs==unique_drugs[drug_ind, :])).sum(axis=1) == 0)[0])\n",
    "\n",
    "train_inds = np.delete(np.arange(all_train_drugs.shape[0]), val_inds)\n",
    "\n",
    "    \n",
    "XD_train, XD_val = all_train_drugs[train_inds, :], all_train_drugs[val_inds, :]\n",
    "XT_train, XT_val = all_train_prots[train_inds, :], all_train_prots[val_inds, :]\n",
    "Y_train, Y_val = all_train_Y[train_inds], all_train_Y[val_inds]\n",
    "\n",
    "#XD_train, XD_val, XT_train, XT_val, Y_train, Y_val = train_test_split(all_train_drugs, all_train_prots, all_train_Y, \n",
    "#                                                                      test_size=0.1, \n",
    "#                                                                     random_state=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ibrahim/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ibrahim/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 53955 samples, validate on 3151 samples\n",
      "Epoch 1/1000\n",
      "Epoch 00001: val_loss improved from inf to 1.87669, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 38s - loss: 1.3416 - cindex_score: 0.6434 - val_loss: 1.8767 - val_cindex_score: 0.3828\n",
      "Epoch 2/1000\n",
      "Epoch 00002: val_loss improved from 1.87669 to 1.38942, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.5921 - cindex_score: 0.6925 - val_loss: 1.3894 - val_cindex_score: 0.3797\n",
      "Epoch 3/1000\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 32s - loss: 0.5335 - cindex_score: 0.7202 - val_loss: 2.1693 - val_cindex_score: 0.5163\n",
      "Epoch 4/1000\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 32s - loss: 0.5156 - cindex_score: 0.7262 - val_loss: 2.4572 - val_cindex_score: 0.6314\n",
      "Epoch 5/1000\n",
      "Epoch 00005: val_loss improved from 1.38942 to 1.21198, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.5001 - cindex_score: 0.7306 - val_loss: 1.2120 - val_cindex_score: 0.6579\n",
      "Epoch 6/1000\n",
      "Epoch 00006: val_loss improved from 1.21198 to 0.93102, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.4714 - cindex_score: 0.7437 - val_loss: 0.9310 - val_cindex_score: 0.7273\n",
      "Epoch 7/1000\n",
      "Epoch 00007: val_loss improved from 0.93102 to 0.81123, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.4571 - cindex_score: 0.7561 - val_loss: 0.8112 - val_cindex_score: 0.7280\n",
      "Epoch 8/1000\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 32s - loss: 0.4341 - cindex_score: 0.7643 - val_loss: 0.8571 - val_cindex_score: 0.7067\n",
      "Epoch 9/1000\n",
      "Epoch 00009: val_loss improved from 0.81123 to 0.80363, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.4407 - cindex_score: 0.7603 - val_loss: 0.8036 - val_cindex_score: 0.7495\n",
      "Epoch 10/1000\n",
      "Epoch 00010: val_loss improved from 0.80363 to 0.72533, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.4209 - cindex_score: 0.7669 - val_loss: 0.7253 - val_cindex_score: 0.7645\n",
      "Epoch 11/1000\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 32s - loss: 0.4227 - cindex_score: 0.7682 - val_loss: 1.1491 - val_cindex_score: 0.7220\n",
      "Epoch 12/1000\n",
      "Epoch 00012: val_loss improved from 0.72533 to 0.71740, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.4122 - cindex_score: 0.7751 - val_loss: 0.7174 - val_cindex_score: 0.7571\n",
      "Epoch 13/1000\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 32s - loss: 0.4028 - cindex_score: 0.7795 - val_loss: 0.8290 - val_cindex_score: 0.7556\n",
      "Epoch 14/1000\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 32s - loss: 0.4038 - cindex_score: 0.7793 - val_loss: 0.8789 - val_cindex_score: 0.7585\n",
      "Epoch 15/1000\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 32s - loss: 0.4008 - cindex_score: 0.7825 - val_loss: 0.8367 - val_cindex_score: 0.7448\n",
      "Epoch 16/1000\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 32s - loss: 0.3878 - cindex_score: 0.7882 - val_loss: 0.9021 - val_cindex_score: 0.7493\n",
      "Epoch 17/1000\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 32s - loss: 0.3910 - cindex_score: 0.7853 - val_loss: 0.8119 - val_cindex_score: 0.7527\n",
      "Epoch 18/1000\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 32s - loss: 0.3937 - cindex_score: 0.7841 - val_loss: 0.7580 - val_cindex_score: 0.7818\n",
      "Epoch 19/1000\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 32s - loss: 0.3919 - cindex_score: 0.7849 - val_loss: 0.7967 - val_cindex_score: 0.7652\n",
      "Epoch 20/1000\n",
      "Epoch 00020: val_loss improved from 0.71740 to 0.65599, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.3887 - cindex_score: 0.7892 - val_loss: 0.6560 - val_cindex_score: 0.7675\n",
      "Epoch 21/1000\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 32s - loss: 0.3815 - cindex_score: 0.7913 - val_loss: 0.7447 - val_cindex_score: 0.7414\n",
      "Epoch 22/1000\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 32s - loss: 0.3766 - cindex_score: 0.7938 - val_loss: 0.8474 - val_cindex_score: 0.7030\n",
      "Epoch 23/1000\n",
      "Epoch 00023: val_loss improved from 0.65599 to 0.63537, saving model to checkpoints/davis_dtc_dta.h5\n",
      " - 32s - loss: 0.3855 - cindex_score: 0.7917 - val_loss: 0.6354 - val_cindex_score: 0.7627\n",
      "Epoch 24/1000\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 32s - loss: 0.3748 - cindex_score: 0.7960 - val_loss: 0.7264 - val_cindex_score: 0.7609\n",
      "Epoch 25/1000\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 32s - loss: 0.3722 - cindex_score: 0.7971 - val_loss: 0.8713 - val_cindex_score: 0.7568\n",
      "Epoch 26/1000\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 32s - loss: 0.3717 - cindex_score: 0.7967 - val_loss: 0.7876 - val_cindex_score: 0.7468\n",
      "Epoch 27/1000\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 32s - loss: 0.3677 - cindex_score: 0.7994 - val_loss: 0.8132 - val_cindex_score: 0.7605\n",
      "Epoch 28/1000\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 32s - loss: 0.3771 - cindex_score: 0.7945 - val_loss: 0.7545 - val_cindex_score: 0.7729\n",
      "Epoch 29/1000\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 32s - loss: 0.3724 - cindex_score: 0.7991 - val_loss: 0.7793 - val_cindex_score: 0.7654\n",
      "Epoch 30/1000\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 32s - loss: 0.3741 - cindex_score: 0.7970 - val_loss: 0.8511 - val_cindex_score: 0.7645\n",
      "Epoch 31/1000\n",
      "Epoch 00031: val_loss did not improve\n",
      " - 32s - loss: 0.3776 - cindex_score: 0.7958 - val_loss: 0.7404 - val_cindex_score: 0.7711\n",
      "Epoch 32/1000\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 32s - loss: 0.3770 - cindex_score: 0.7974 - val_loss: 0.7019 - val_cindex_score: 0.7515\n",
      "Epoch 33/1000\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 32s - loss: 0.3651 - cindex_score: 0.8026 - val_loss: 0.7646 - val_cindex_score: 0.7769\n",
      "Epoch 34/1000\n",
      "Epoch 00034: val_loss did not improve\n",
      " - 32s - loss: 0.3667 - cindex_score: 0.8015 - val_loss: 0.6836 - val_cindex_score: 0.7904\n",
      "Epoch 35/1000\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 32s - loss: 0.3702 - cindex_score: 0.8022 - val_loss: 0.8742 - val_cindex_score: 0.7678\n",
      "Epoch 36/1000\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 32s - loss: 0.3698 - cindex_score: 0.8036 - val_loss: 0.7561 - val_cindex_score: 0.7518\n",
      "Epoch 37/1000\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 32s - loss: 0.3664 - cindex_score: 0.8029 - val_loss: 0.7938 - val_cindex_score: 0.7621\n",
      "Epoch 38/1000\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 32s - loss: 0.3650 - cindex_score: 0.8028 - val_loss: 0.6895 - val_cindex_score: 0.7562\n",
      "Epoch 39/1000\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 32s - loss: 0.3606 - cindex_score: 0.8046 - val_loss: 0.7209 - val_cindex_score: 0.7600\n",
      "Epoch 40/1000\n",
      "Epoch 00040: val_loss did not improve\n",
      " - 32s - loss: 0.3619 - cindex_score: 0.8061 - val_loss: 0.8057 - val_cindex_score: 0.7537\n",
      "Epoch 41/1000\n",
      "Epoch 00041: val_loss did not improve\n",
      " - 32s - loss: 0.3595 - cindex_score: 0.8075 - val_loss: 0.8018 - val_cindex_score: 0.7473\n",
      "Epoch 42/1000\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 32s - loss: 0.3583 - cindex_score: 0.8061 - val_loss: 0.8584 - val_cindex_score: 0.7621\n",
      "Epoch 43/1000\n",
      "Epoch 00043: val_loss did not improve\n",
      " - 32s - loss: 0.3577 - cindex_score: 0.8081 - val_loss: 0.8362 - val_cindex_score: 0.7692\n",
      "Epoch 44/1000\n",
      "Epoch 00044: val_loss did not improve\n",
      " - 32s - loss: 0.3605 - cindex_score: 0.8043 - val_loss: 0.7726 - val_cindex_score: 0.7547\n",
      "Epoch 45/1000\n",
      "Epoch 00045: val_loss did not improve\n",
      " - 32s - loss: 0.3575 - cindex_score: 0.8074 - val_loss: 0.7584 - val_cindex_score: 0.7554\n",
      "Epoch 46/1000\n",
      "Epoch 00046: val_loss did not improve\n",
      " - 32s - loss: 0.3617 - cindex_score: 0.8037 - val_loss: 0.7275 - val_cindex_score: 0.7688\n",
      "Epoch 47/1000\n",
      "Epoch 00047: val_loss did not improve\n",
      " - 32s - loss: 0.3569 - cindex_score: 0.8076 - val_loss: 0.7221 - val_cindex_score: 0.7440\n",
      "Epoch 48/1000\n",
      "Epoch 00048: val_loss did not improve\n",
      " - 32s - loss: 0.3570 - cindex_score: 0.8082 - val_loss: 0.7254 - val_cindex_score: 0.7501\n"
     ]
    }
   ],
   "source": [
    "model_name='checkpoints/davis_dtc_dta'\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=25)\n",
    "checkpoint_callback = ModelCheckpoint(model_name+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "gridmodel = build_combined_categorical(FLAGS, FLAGS.num_windows, FLAGS.smi_window_lengths[0], FLAGS.seq_window_lengths[0])\n",
    "gridres = gridmodel.fit(([XD_train, XT_train ]), Y_train, batch_size=FLAGS.batch_size, epochs=FLAGS.num_epoch, \n",
    "        validation_data=( ([np.array(XD_val), np.array(XT_val) ]), np.array(Y_val))\n",
    "           , callbacks=[early_stopping_callback, checkpoint_callback], verbose=2)\n",
    "\n",
    "gridmodel.save('davis_dtc_dta.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, auc, f1_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "gridmodel = load_model('checkpoints/davis_dtc_dta.h5')\n",
    "predicted_labels = gridmodel.predict([np.array(XD_val), np.array(XT_val) ])\n",
    "loss, rperf2 = gridmodel.evaluate(([np.array(XD_val),np.array(XT_val) ]), np.array(Y_val), verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cindex: [0.82653753]\n",
      "rmse: 0.7971002447060451\n",
      "f1: 0.6114717550558165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "predicted_labels = gridmodel.predict([np.array(XD_val), np.array(XT_val) ])\n",
    "print('cindex:', get_cindex(Y_val, predicted_labels))\n",
    "print('rmse:', np.sqrt(mean_squared_error(Y_val, predicted_labels)))\n",
    "#print('pearsonr:', pearsonr(Y_val, predicted_labels[:, 0]))\n",
    "#print('spearmanr:', np.sqrt(spearmanr(val_Y, predicted_labels[:, 0])))\n",
    "print('f1:', np.sqrt(f1_score(Y_val>7, predicted_labels>7)))\n",
    "\n",
    "#v0.1\n",
    "#cindex: [0.81675458]\n",
    "#rmse: 0.8683109279804\n",
    "#f1: 0.6599120175960898\n",
    "#checkpoint\n",
    "#cindex: [0.79595819]\n",
    "#rmse: 0.7753355061972348\n",
    "#f1: 0.6260990336999411\n",
    "\n",
    "#v0.2\n",
    "#cindex: [0.82653753]\n",
    "#rmse: 0.7971002447060451\n",
    "#f1: 0.6114717550558165\n",
    "#checkpoint\n",
    "#cindex: [0.82653753]\n",
    "#rmse: 0.7971002447060451\n",
    "#f1: 0.6114717550558165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XD_test, XT_test = get_DTC_train(data_path+'dtc_test_for_deepDTA.csv', FLAGS.max_smi_len, FLAGS.max_seq_len, with_label=False)\n",
    "XD_test, XT_test = np.asarray(XD_test), np.asarray(XT_test)\n",
    "predicted_labels = gridmodel.predict([np.array(XD_test), np.array(XT_test) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.read_csv(data_path+'round_1_template.csv')\n",
    "submission_data.loc[:, 'pKd_[M]_pred'] = predicted_labels\n",
    "submission_data.to_csv(data_path+'submission_file.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
